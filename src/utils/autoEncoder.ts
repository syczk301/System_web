import * as tf from '@tensorflow/tfjs';
import { Matrix } from 'ml-matrix';

// è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
// tf.util.seedrandom('42'); // è¿™ä¸ªAPIä¸å­˜åœ¨ï¼Œåˆ é™¤è¿™è¡Œ

export interface AEResults {
  analyzer: AEAnalyzer;
  data: number[][];
  X_train: number[][];
  X_test: number[][];
  re2_test: number[];
  spe_test: number[];
  re2_control_limit: number;
  spe_control_limit: number;
  re2_anomalies: AnomalyResult;
  spe_anomalies: AnomalyResult;
  train_losses: number[];
}

export interface AnomalyResult {
  mask: boolean[];
  indices: number[];
  count: number;
  percentage: number;
}

export interface TrainingProgress {
  epoch: number;
  totalEpochs: number;
  loss: number;
  message: string;
}

export type ProgressCallback = (message: string) => void;

/**
 * æ ‡å‡†åŒ–å™¨ç±» - æ¨¡æ‹Ÿsklearnçš„StandardScaler
 */
export class StandardScaler {
  private mean: number[] = [];
  private std: number[] = [];
  private fitted = false;

  fit(data: number[][]): void {
    const matrix = new Matrix(data);
    this.mean = [];
    this.std = [];

    for (let col = 0; col < matrix.columns; col++) {
      const column = matrix.getColumn(col);
      const mean = column.reduce((a, b) => a + b, 0) / column.length;
      const variance = column.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / column.length;
      const std = Math.sqrt(variance);

      this.mean.push(mean);
      this.std.push(std || 1); // é¿å…é™¤é›¶
    }
    this.fitted = true;
  }

  transform(data: number[][]): number[][] {
    if (!this.fitted) {
      throw new Error('Scaler must be fitted before transform');
    }

    return data.map(row =>
      row.map((val, idx) => (val - this.mean[idx]) / this.std[idx])
    );
  }

  fitTransform(data: number[][]): number[][] {
    this.fit(data);
    return this.transform(data);
  }
}

/**
 * è‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹ç±» - åŸºäºTensorFlow.js
 */
export class AutoEncoder {
  private model: tf.LayersModel | null = null;
  private inputDim: number;
  private encodingDim: number;

  constructor(inputDim: number, encodingDim: number = 10) {
    this.inputDim = inputDim;
    this.encodingDim = encodingDim;
    this.buildModel();
  }

  private buildModel(): void {
    // è¾“å…¥å±‚
    const input = tf.input({ shape: [this.inputDim] });

    // ç¼–ç å™¨éƒ¨åˆ†
    const encoded1 = tf.layers.dense({
      units: 64,
      activation: 'relu',
      kernelInitializer: 'glorotUniform'
    }).apply(input);

    const encoded2 = tf.layers.dense({
      units: 32,
      activation: 'relu',
      kernelInitializer: 'glorotUniform'
    }).apply(encoded1);

    const encoded3 = tf.layers.dense({
      units: this.encodingDim,
      activation: 'relu',
      kernelInitializer: 'glorotUniform'
    }).apply(encoded2);

    // è§£ç å™¨éƒ¨åˆ†
    const decoded1 = tf.layers.dense({
      units: 32,
      activation: 'relu',
      kernelInitializer: 'glorotUniform'
    }).apply(encoded3);

    const decoded2 = tf.layers.dense({
      units: 64,
      activation: 'relu',
      kernelInitializer: 'glorotUniform'
    }).apply(decoded1);

    const decoded3 = tf.layers.dense({
      units: this.inputDim,
      activation: 'sigmoid',
      kernelInitializer: 'glorotUniform'
    }).apply(decoded2);

    // åˆ›å»ºæ¨¡å‹
    this.model = tf.model({ inputs: input, outputs: decoded3 as tf.SymbolicTensor });

    // ç¼–è¯‘æ¨¡å‹
    this.model.compile({
      optimizer: tf.train.adam(0.001),
      loss: 'meanSquaredError'
    });
  }

  async predict(data: tf.Tensor): Promise<tf.Tensor> {
    if (!this.model) {
      throw new Error('Model not built');
    }
    return this.model.predict(data) as tf.Tensor;
  }

  async fit(
    xTrain: tf.Tensor,
    epochs: number = 150,
    batchSize: number = 32,
    onEpochEnd?: (epoch: number, loss: number) => void
  ): Promise<number[]> {
    if (!this.model) {
      throw new Error('Model not built');
    }

    const losses: number[] = [];

    const history = await this.model.fit(xTrain, xTrain, {
      epochs,
      batchSize,
      shuffle: true,
      callbacks: {
        onEpochEnd: (epoch, logs) => {
          const loss = logs?.loss as number;
          losses.push(loss);
          if (onEpochEnd) {
            onEpochEnd(epoch, loss);
          }
        }
      }
    });

    return losses;
  }

  dispose(): void {
    if (this.model) {
      this.model.dispose();
    }
  }
}

/**
 * è‡ªåŠ¨ç¼–ç å™¨åˆ†æå™¨ç±»
 */
export class AEAnalyzer {
  private model: AutoEncoder | null = null;
  private scaler: StandardScaler = new StandardScaler();
  private isTrained = false;
  private trainLosses: number[] = [];

  /**
   * åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
   */
  loadData(data: number[][]): number[][] {
    try {
      // è¿‡æ»¤æ‰åŒ…å«NaNæˆ–æ— ç©·å¤§çš„è¡Œ
      const cleanData = data.filter(row => 
        row.every(val => !isNaN(val) && isFinite(val))
      );

      if (cleanData.length === 0) {
        throw new Error('æ•°æ®ä¸ºç©ºæˆ–æ²¡æœ‰æœ‰æ•ˆçš„æ•°å€¼');
      }

      console.log(`æ•°æ®åŠ è½½æˆåŠŸ: ${cleanData.length} è¡Œ, ${cleanData[0].length} åˆ—`);
      return cleanData;
    } catch (error) {
      console.error('æ•°æ®åŠ è½½å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * æ•°æ®é¢„å¤„ç† - åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
   */
  preprocessData(data: number[][], testSize: number = 0.2): {
    XTrain: number[][];
    XTest: number[][];
  } {
    try {
      // æ ‡å‡†åŒ–æ•°æ®
      const XScaled = this.scaler.fitTransform(data);

      // ç®€å•çš„è®­ç»ƒæµ‹è¯•é›†åˆ†å‰²
      const trainSize = Math.floor(XScaled.length * (1 - testSize));
      const shuffledIndices = Array.from({ length: XScaled.length }, (_, i) => i)
        .sort(() => Math.random() - 0.5);

      const trainIndices = shuffledIndices.slice(0, trainSize);
      const testIndices = shuffledIndices.slice(trainSize);

      const XTrain = trainIndices.map(i => XScaled[i]);
      const XTest = testIndices.map(i => XScaled[i]);

      console.log(`æ•°æ®é¢„å¤„ç†å®Œæˆ - è®­ç»ƒé›†: ${XTrain.length}, æµ‹è¯•é›†: ${XTest.length}`);
      return { XTrain, XTest };
    } catch (error) {
      console.error('æ•°æ®é¢„å¤„ç†å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹
   */
  async trainModel(
    XTrain: number[][],
    XTest: number[][],
    epochs: number = 150,
    batchSize: number = 32,
    learningRate: number = 0.001,
    progressCallback?: ProgressCallback
  ): Promise<boolean> {
    try {
      const inputDim = XTrain[0].length;
      const encodingDim = Math.max(2, Math.floor(inputDim / 4));

      // åˆ›å»ºæ¨¡å‹
      this.model = new AutoEncoder(inputDim, encodingDim);

      if (progressCallback) {
        progressCallback('ğŸš€ å¼€å§‹è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹...');
        progressCallback(`ğŸ“Š æ¨¡å‹ç»“æ„: ${inputDim} -> ${encodingDim} -> ${inputDim}`);
        progressCallback(`âš™ï¸ è®­ç»ƒå‚æ•°: epochs=${epochs}, batch_size=${batchSize}, lr=${learningRate}`);
        progressCallback('-'.repeat(50));
      }

      // è½¬æ¢ä¸ºTensorFlow.jså¼ é‡
      const xTrainTensor = tf.tensor2d(XTrain);

      // è®­ç»ƒæ¨¡å‹
      this.trainLosses = await this.model.fit(
        xTrainTensor,
        epochs,
        batchSize,
        (epoch, loss) => {
          if (progressCallback) {
            progressCallback(`è®­ç»ƒè½®æ¬¡ ${epoch + 1}/${epochs}, æŸå¤±: ${loss.toFixed(6)}`);
          }
        }
      );

      this.isTrained = true;

      if (progressCallback) {
        progressCallback('-'.repeat(50));
        progressCallback('âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!');
        progressCallback(`ğŸ“ˆ æœ€ç»ˆæŸå¤±: ${this.trainLosses[this.trainLosses.length - 1].toFixed(6)}`);
      }

      // æ¸…ç†å¼ é‡
      xTrainTensor.dispose();

      return true;
    } catch (error) {
      if (progressCallback) {
        progressCallback(`âŒ è®­ç»ƒå¤±è´¥: ${error}`);
      }
      console.error('è®­ç»ƒå¤±è´¥:', error);
      return false;
    }
  }

  /**
   * è®¡ç®—REÂ²å’ŒSPEç»Ÿè®¡é‡
   */
  async calculateStatistics(XData: number[][]): Promise<{
    re2: number[];
    spe: number[];
  }> {
    if (!this.isTrained || !this.model) {
      throw new Error('æ¨¡å‹å°šæœªè®­ç»ƒ');
    }

    const xDataTensor = tf.tensor2d(XData);
    
    try {
      // è·å–é‡æ„æ•°æ®
      const xReconstructed = await this.model.predict(xDataTensor);
      
      // è®¡ç®—é‡æ„è¯¯å·®
      const reconstructionError = tf.sub(xDataTensor, xReconstructed);
      
      // è®¡ç®—SPE (å¹³æ–¹é¢„æµ‹è¯¯å·®)
      const spe = tf.sum(tf.square(reconstructionError), 1);
      
      // è®¡ç®—REÂ² (æœ€å¤§é‡æ„è¯¯å·®çš„å¹³æ–¹)
      const re2 = tf.max(tf.square(reconstructionError), 1);
      
      // è½¬æ¢ä¸ºJavaScriptæ•°ç»„
      const speArray = await spe.data();
      const re2Array = await re2.data();
      
      // æ¸…ç†å¼ é‡
      xDataTensor.dispose();
      xReconstructed.dispose();
      reconstructionError.dispose();
      spe.dispose();
      re2.dispose();
      
      return {
        re2: Array.from(re2Array),
        spe: Array.from(speArray)
      };
    } catch (error) {
      xDataTensor.dispose();
      throw error;
    }
  }

  /**
   * è®¡ç®—æ§åˆ¶é™
   */
  calculateControlLimits(statistics: number[], confidenceLevel: number = 0.99): number {
    const sorted = [...statistics].sort((a, b) => a - b);
    const index = Math.floor(sorted.length * confidenceLevel);
    return sorted[Math.min(index, sorted.length - 1)];
  }

  /**
   * æ£€æµ‹å¼‚å¸¸
   */
  detectAnomalies(statistics: number[], controlLimit: number): AnomalyResult {
    const anomalyMask = statistics.map(stat => stat > controlLimit);
    const anomalyIndices = anomalyMask
      .map((isAnomaly, index) => isAnomaly ? index : -1)
      .filter(index => index !== -1);
    
    const anomalyCount = anomalyIndices.length;
    const anomalyPercentage = (anomalyCount / statistics.length) * 100;

    return {
      mask: anomalyMask,
      indices: anomalyIndices,
      count: anomalyCount,
      percentage: anomalyPercentage
    };
  }

  /**
   * æ¸…ç†èµ„æº
   */
  dispose(): void {
    if (this.model) {
      this.model.dispose();
    }
  }

  get isTrainedModel(): boolean {
    return this.isTrained;
  }

  get losses(): number[] {
    return [...this.trainLosses];
  }
}

/**
 * è¿è¡Œæ•…éšœæ£€æµ‹åˆ†æçš„ä¸»å‡½æ•°
 */
export async function runFaultDetection(
  data: number[][],
  progressCallback?: ProgressCallback,
  epochs: number = 150
): Promise<AEResults | null> {
  try {
    // åˆå§‹åŒ–åˆ†æå™¨
    const analyzer = new AEAnalyzer();

    if (progressCallback) {
      progressCallback('ğŸ“ æ­£åœ¨å¤„ç†æ•°æ®...');
    }

    // åŠ è½½æ•°æ®
    const cleanData = analyzer.loadData(data);

    if (progressCallback) {
      progressCallback(`âœ… æ•°æ®å¤„ç†æˆåŠŸ: ${cleanData.length} è¡Œ, ${cleanData[0].length} åˆ—`);
      progressCallback('ğŸ”„ æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†...');
    }

    // é¢„å¤„ç†æ•°æ®
    const { XTrain, XTest } = analyzer.preprocessData(cleanData);

    if (progressCallback) {
      progressCallback('âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ');
    }

    // è®­ç»ƒæ¨¡å‹
    const success = await analyzer.trainModel(XTrain, XTest, epochs, 32, 0.001, progressCallback);
    if (!success) {
      return null;
    }

    if (progressCallback) {
      progressCallback('ğŸ“Š æ­£åœ¨è®¡ç®—ç»Ÿè®¡é‡å’Œæ§åˆ¶é™...');
    }

    // è®¡ç®—æµ‹è¯•é›†ç»Ÿè®¡é‡
    const { re2: re2Test, spe: speTest } = await analyzer.calculateStatistics(XTest);

    // è®¡ç®—æ§åˆ¶é™
    const re2ControlLimit = analyzer.calculateControlLimits(re2Test);
    const speControlLimit = analyzer.calculateControlLimits(speTest);

    // æ£€æµ‹å¼‚å¸¸
    const re2Anomalies = analyzer.detectAnomalies(re2Test, re2ControlLimit);
    const speAnomalies = analyzer.detectAnomalies(speTest, speControlLimit);

    if (progressCallback) {
      progressCallback('âœ… ç»Ÿè®¡é‡è®¡ç®—å®Œæˆ');
      progressCallback(`ğŸ” REÂ² å¼‚å¸¸æ£€æµ‹: ${re2Anomalies.count} ä¸ªå¼‚å¸¸æ ·æœ¬ (${re2Anomalies.percentage.toFixed(2)}%)`);
      progressCallback(`ğŸ” SPE å¼‚å¸¸æ£€æµ‹: ${speAnomalies.count} ä¸ªå¼‚å¸¸æ ·æœ¬ (${speAnomalies.percentage.toFixed(2)}%)`);
      progressCallback('ğŸ‰ åˆ†æå®Œæˆ!');
    }

    // è¿”å›ç»“æœ
    const results: AEResults = {
      analyzer,
      data: cleanData,
      X_train: XTrain,
      X_test: XTest,
      re2_test: re2Test,
      spe_test: speTest,
      re2_control_limit: re2ControlLimit,
      spe_control_limit: speControlLimit,
      re2_anomalies: re2Anomalies,
      spe_anomalies: speAnomalies,
      train_losses: analyzer.losses
    };

    return results;
  } catch (error) {
    if (progressCallback) {
      progressCallback(`âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: ${error}`);
    }
    console.error('æ•…éšœæ£€æµ‹åˆ†æå¤±è´¥:', error);
    return null;
  }
}

/**
 * è¿è¡ŒREÂ²åˆ†æ
 */
export async function runAeRe2Analysis(
  data: number[][],
  progressCallback?: ProgressCallback,
  epochs: number = 150
): Promise<AEResults | null> {
  return runFaultDetection(data, progressCallback, epochs);
}

/**
 * è¿è¡ŒSPEåˆ†æ
 */
export async function runAeSpeAnalysis(
  data: number[][],
  progressCallback?: ProgressCallback,
  epochs: number = 150
): Promise<AEResults | null> {
  return runFaultDetection(data, progressCallback, epochs);
} 