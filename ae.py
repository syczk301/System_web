import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from scipy.stats import chi2
import os
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
torch.manual_seed(42)
np.random.seed(42)

class AutoEncoder(nn.Module):
    """åŸºäºPyTorchçš„è‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹"""
    
    def __init__(self, input_dim, encoding_dim=10):
        super(AutoEncoder, self).__init__()
        
        # ç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, encoding_dim),
            nn.ReLU()
        )
        
        # è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 64),
            nn.ReLU(),
            nn.Linear(64, input_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded
    
    def encode(self, x):
        return self.encoder(x)

class AEAnalyzer:
    """è‡ªåŠ¨ç¼–ç å™¨åˆ†æå™¨"""
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.is_trained = False
        self.train_losses = []
        
    def load_data(self, file_path):
        """åŠ è½½æ•°æ®"""
        try:
            if file_path.endswith('.xlsx'):
                data = pd.read_excel(file_path, decimal=',')
            else:
                data = pd.read_csv(file_path)
            
            # åªä¿ç•™æ•°å€¼åˆ—
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            data = data[numeric_columns]
            
            # å»é™¤ç¼ºå¤±å€¼
            data = data.dropna()
            
            if data.empty:
                raise ValueError("æ•°æ®ä¸ºç©ºæˆ–æ²¡æœ‰æ•°å€¼åˆ—")
            
            print(f"æ•°æ®åŠ è½½æˆåŠŸ: {data.shape[0]} è¡Œ, {data.shape[1]} åˆ—")
            return data
            
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def preprocess_data(self, data, test_size=0.2):
        """æ•°æ®é¢„å¤„ç†"""
        try:
            # æ ‡å‡†åŒ–
            X_scaled = self.scaler.fit_transform(data)
            
            # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            X_train, X_test = train_test_split(X_scaled, test_size=test_size, random_state=42)
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            X_train_tensor = torch.FloatTensor(X_train).to(self.device)
            X_test_tensor = torch.FloatTensor(X_test).to(self.device)
            
            print(f"æ•°æ®é¢„å¤„ç†å®Œæˆ - è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
            return X_train_tensor, X_test_tensor
            
        except Exception as e:
            print(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}")
            return None, None
    
    def train_model(self, X_train, X_test, epochs=150, batch_size=32, learning_rate=0.001, progress_callback=None):
        """è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹"""
        try:
            input_dim = X_train.shape[1]
            encoding_dim = max(2, input_dim // 4)  # ç¼–ç ç»´åº¦ä¸ºè¾“å…¥ç»´åº¦çš„1/4
            
            # åˆ›å»ºæ¨¡å‹
            self.model = AutoEncoder(input_dim, encoding_dim).to(self.device)
            
            # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
            criterion = nn.MSELoss()
            optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_dataset = TensorDataset(X_train, X_train)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            
            self.train_losses = []
            
            if progress_callback:
                progress_callback("ğŸš€ å¼€å§‹è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹...")
                progress_callback(f"ğŸ“Š æ¨¡å‹ç»“æ„: {input_dim} -> {encoding_dim} -> {input_dim}")
                progress_callback(f"âš™ï¸ è®­ç»ƒå‚æ•°: epochs={epochs}, batch_size={batch_size}, lr={learning_rate}")
                progress_callback("-" * 50)
            
            # è®­ç»ƒå¾ªç¯
            self.model.train()
            for epoch in range(epochs):
                epoch_loss = 0.0
                for batch_idx, (data, target) in enumerate(train_loader):
                    optimizer.zero_grad()
                    # å‰å‘ä¼ æ’­
                    output = self.model(data)
                    loss = criterion(output, target)
                    # åå‘ä¼ æ’­
                    loss.backward()
                    optimizer.step()
                    epoch_loss += loss.item()
                # è®¡ç®—å¹³å‡æŸå¤±
                avg_loss = epoch_loss / len(train_loader)
                self.train_losses.append(avg_loss)
                # æ›´æ–°è¿›åº¦ï¼ˆæ¯ä¸€è½®éƒ½è®°å½•ï¼‰
                if progress_callback:
                    progress_callback(f"è®­ç»ƒè½®æ¬¡ {epoch + 1}/{epochs}, æŸå¤±: {avg_loss:.6f}")
            
            self.is_trained = True
            
            if progress_callback:
                progress_callback("-" * 50)
                progress_callback("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
                progress_callback(f"ğŸ“ˆ æœ€ç»ˆæŸå¤±: {self.train_losses[-1]:.6f}")
            
            return True
            
        except Exception as e:
            if progress_callback:
                progress_callback(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
            print(f"è®­ç»ƒå¤±è´¥: {str(e)}")
            return False
    
    def calculate_statistics(self, X_data):
        """è®¡ç®—REÂ²å’ŒSPEç»Ÿè®¡é‡"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        self.model.eval()
        with torch.no_grad():
            # è·å–é‡æ„æ•°æ®
            X_reconstructed = self.model(X_data)
            
            # è®¡ç®—é‡æ„è¯¯å·®
            reconstruction_error = X_data - X_reconstructed
            
            # è®¡ç®—SPE (å¹³æ–¹é¢„æµ‹è¯¯å·®)
            spe = torch.sum(reconstruction_error ** 2, dim=1).cpu().numpy()
            
            # è®¡ç®—REÂ² (æœ€å¤§é‡æ„è¯¯å·®çš„å¹³æ–¹)
            re2 = torch.max(reconstruction_error ** 2, dim=1)[0].cpu().numpy()
            
        return re2, spe
    
    def calculate_control_limits(self, statistics, confidence_level=0.99):
        """è®¡ç®—æ§åˆ¶é™"""
        # ä½¿ç”¨ç»éªŒåˆ†ä½æ•°æ–¹æ³•
        control_limit = np.percentile(statistics, confidence_level * 100)
        return control_limit
    
    def detect_anomalies(self, statistics, control_limit):
        """æ£€æµ‹å¼‚å¸¸"""
        anomaly_mask = statistics > control_limit
        anomaly_indices = np.where(anomaly_mask)[0]
        anomaly_count = len(anomaly_indices)
        anomaly_percentage = (anomaly_count / len(statistics)) * 100
        
        return {
            'mask': anomaly_mask,
            'indices': anomaly_indices,
            'count': anomaly_count,
            'percentage': anomaly_percentage
        }

def run_fault_detection(file_path, progress_callback=None, epochs=150):
    """è¿è¡Œæ•…éšœæ£€æµ‹åˆ†æ"""
    try:
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = AEAnalyzer()
        
        if progress_callback:
            progress_callback("ğŸ“ æ­£åœ¨åŠ è½½æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        data = analyzer.load_data(file_path)
        if data is None:
            return None
        
        if progress_callback:
            progress_callback(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {data.shape[0]} è¡Œ, {data.shape[1]} åˆ—")
            progress_callback("ğŸ”„ æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†...")
        
        # é¢„å¤„ç†æ•°æ®
        X_train, X_test = analyzer.preprocess_data(data)
        if X_train is None:
            return None
        
        if progress_callback:
            progress_callback("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        
        # è®­ç»ƒæ¨¡å‹
        success = analyzer.train_model(X_train, X_test, epochs=epochs, progress_callback=progress_callback)
        if not success:
            return None
        
        if progress_callback:
            progress_callback("ğŸ“Š æ­£åœ¨è®¡ç®—ç»Ÿè®¡é‡å’Œæ§åˆ¶é™...")
        
        # è®¡ç®—æµ‹è¯•é›†ç»Ÿè®¡é‡
        re2_test, spe_test = analyzer.calculate_statistics(X_test)
        
        # è®¡ç®—æ§åˆ¶é™
        re2_control_limit = analyzer.calculate_control_limits(re2_test)
        spe_control_limit = analyzer.calculate_control_limits(spe_test)
        
        # æ£€æµ‹å¼‚å¸¸
        re2_anomalies = analyzer.detect_anomalies(re2_test, re2_control_limit)
        spe_anomalies = analyzer.detect_anomalies(spe_test, spe_control_limit)
        
        if progress_callback:
            progress_callback("âœ… ç»Ÿè®¡é‡è®¡ç®—å®Œæˆ")
            progress_callback(f"ğŸ” REÂ² å¼‚å¸¸æ£€æµ‹: {re2_anomalies['count']} ä¸ªå¼‚å¸¸æ ·æœ¬ ({re2_anomalies['percentage']:.2f}%)")
            progress_callback(f"ğŸ” SPE å¼‚å¸¸æ£€æµ‹: {spe_anomalies['count']} ä¸ªå¼‚å¸¸æ ·æœ¬ ({spe_anomalies['percentage']:.2f}%)")
            progress_callback("ğŸ‰ åˆ†æå®Œæˆ!")
        
        # è¿”å›ç»“æœ
        results = {
            'analyzer': analyzer,
            'data': data,
            'X_train': X_train.cpu().numpy(),
            'X_test': X_test.cpu().numpy(),
            're2_test': re2_test,
            'spe_test': spe_test,
            're2_control_limit': re2_control_limit,
            'spe_control_limit': spe_control_limit,
            're2_anomalies': re2_anomalies,
            'spe_anomalies': spe_anomalies,
            'train_losses': analyzer.train_losses
        }
        
        return results
        
    except Exception as e:
        if progress_callback:
            progress_callback(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
        print(f"æ•…éšœæ£€æµ‹åˆ†æå¤±è´¥: {str(e)}")
        return None

def run_ae_re2_analysis(file_path, progress_callback=None, epochs=150):
    """è¿è¡ŒREÂ²åˆ†æ"""
    return run_fault_detection(file_path, progress_callback, epochs)

def run_ae_spe_analysis(file_path, progress_callback=None, epochs=150):
    """è¿è¡ŒSPEåˆ†æ"""
    return run_fault_detection(file_path, progress_callback, epochs)

# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    def test_callback(message):
        print(message)
    
    # æµ‹è¯•åˆ†æ
    file_path = "../data/æ­£å¸¸æ•°æ®.xlsx"  # æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´
    if os.path.exists(file_path):
        results = run_fault_detection(file_path, test_callback, epochs=50)
        if results:
            print("æµ‹è¯•æˆåŠŸ!")
            print(f"REÂ² æ§åˆ¶é™: {results['re2_control_limit']:.4f}")
            print(f"SPE æ§åˆ¶é™: {results['spe_control_limit']:.4f}")
            print(f"REÂ² å¼‚å¸¸æ•°é‡: {results['re2_anomalies']['count']}")
            print(f"SPE å¼‚å¸¸æ•°é‡: {results['spe_anomalies']['count']}")
        else:
            print("æµ‹è¯•å¤±è´¥!")
    else:
        print(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}") 